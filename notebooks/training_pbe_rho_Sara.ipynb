{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ICN2/snavarro/.local/lib/python3.10/site-packages/pyscf/dft/libxc.py:772: UserWarning: Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, the same to the B3LYP functional in Gaussian and ORCA (issue 1480). To restore the VWN5 definition, you can put the setting \"B3LYP_WITH_VWN5 = True\" in pyscf_conf.py\n",
      "  warnings.warn('Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, '\n",
      "An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import equinox as eqx\n",
    "import optax\n",
    "import numpy as np\n",
    "from pyscf import dft, scf, gto, cc\n",
    "from pyscfad import dft as dft_ad\n",
    "from pyscfad import gto as gto_ad\n",
    "from functools import partial\n",
    "import pylibxc\n",
    "import pyscfad.dft as dftad\n",
    "from jax import custom_jvp\n",
    "jax.config.update(\"jax_enable_x64\", True) #Enables 64 bit precision\n",
    "\n",
    "\n",
    "from xcquinox import net\n",
    "from xcquinox.loss import compute_loss_mae\n",
    "from xcquinox.train import Pretrainer\n",
    "from xcquinox.utils import gen_grid_s, PBE_Fx, PBE_Fc, calculate_stats, lda_x, pw92c_unpolarized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes- r/gr/s: (315,)/(315,)/(315,)\n",
      "shapes- r/gr/s: (315,)/(315,)/(315,)\n"
     ]
    }
   ],
   "source": [
    "#training and validation values\n",
    "inds, vals, tflats, vflats = gen_grid_s(npts = 1e5)\n",
    "train_inds, val_inds = inds\n",
    "rv, grv, sv = vals\n",
    "trf, tgrf, tsf = tflats\n",
    "vrf, vgrf, vsf = vflats\n",
    "#training and validation values for SIGMA\n",
    "sinds, svals, stflats, svflats = gen_grid_s(npts = 1e5, sigma=True)\n",
    "strain_inds, sval_inds = sinds\n",
    "srv, sgrv, ssv = svals\n",
    "strf, stgrf, stsf = stflats\n",
    "svrf, svgrf, svsf = svflats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_fx = PBE_Fx(trf, tgrf)\n",
    "ref_fc = PBE_Fc(trf, tgrf)\n",
    "\n",
    "siginputs = jnp.stack([strf, stgrf], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sigma-based networks\n",
    "spbe_fx = net.GGA_FxNet_sigma(depth=3, nodes=16, seed=92017, lower_rho_cutoff = 0)\n",
    "spbe_fc = net.GGA_FcNet_sigma(depth=3, nodes=16, seed=92017, lower_rho_cutoff = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create optimizers\n",
    "soptimizer_fx = optax.chain(\n",
    "    optax.adam(learning_rate=1e-3)\n",
    ")\n",
    "soptimizer_fc = optax.chain(\n",
    "    optax.adam(learning_rate=1e-3)\n",
    ")\n",
    "spt_pbe_fx = Pretrainer(model = spbe_fx, optim = soptimizer_fx, inputs = siginputs, ref = ref_fx, loss = compute_loss_mae, steps = 2500)\n",
    "spt_pbe_fc = Pretrainer(model = spbe_fc, optim = soptimizer_fc, inputs = siginputs, ref = ref_fc, loss = compute_loss_mae, steps = 2500)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snmx, snlx = spt_pbe_fx()\n",
    "# snmc, snlc = spt_pbe_fc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net.save_xcquinox_model(snmx, path='pretrained_test', loss=snlx)\n",
    "# net.save_xcquinox_model(snmc, path='pretrained_test', loss=snlc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained_test/GGA_FxNet_sigma_d3_n16_s92017.eqx\n",
      "Loaded pretrained_test/GGA_FcNet_sigma_d3_n16_s92017.eqx\n"
     ]
    }
   ],
   "source": [
    "snmx = net.load_xcquinox_model('pretrained_test/GGA_FxNet_sigma_d3_n16_s92017')\n",
    "snmc = net.load_xcquinox_model('pretrained_test/GGA_FcNet_sigma_d3_n16_s92017')\n",
    "snlx = np.loadtxt('pretrained_test/GGA_FxNet_sigma_d3_n16_s92017_loss.txt')\n",
    "snlc = np.loadtxt('pretrained_test/GGA_FcNet_sigma_d3_n16_s92017_loss.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_pbe_Fx(rho, sigma, XNET = None):\n",
    "    #this will be a call to the Fx neural network we want\n",
    "    # print('DEBUG custom_pbe_Fx, rho/sigma shapes: ', rho.shape, sigma.shape)\n",
    "    # print('DEBUG custom_pbe_Fx: rho: ', rho)\n",
    "    # print('DEBUG custom_pbe_Fx: sigma: ', sigma)\n",
    "\n",
    "    Fx = XNET([rho, sigma])\n",
    "    return Fx\n",
    "\n",
    "def custom_pbe_Fc(rho, sigma, CNET = None): #Assumes zeta = 0\n",
    "    #this will be a call to the Fc neural network we want\n",
    "    Fc = CNET([rho, sigma])\n",
    "    return Fc\n",
    "\n",
    "def custom_pbe_e(rho, sigma, XNET = None, CNET = None):\n",
    "    Fx = custom_pbe_Fx(rho, sigma, XNET = XNET)\n",
    "    Fc = custom_pbe_Fc(rho, sigma, CNET = CNET)\n",
    "\n",
    "    exc = lda_x(rho)*Fx + pw92c_unpolarized(rho)*Fc\n",
    "\n",
    "    return exc\n",
    "\n",
    "def custom_pbe_epsilon(rho, sigma, XNET = None, CNET = None):\n",
    "\n",
    "    return rho*custom_pbe_e(rho, sigma, XNET = XNET, CNET = CNET)\n",
    "\n",
    "def derivable_custom_pbe_e(rhosigma, XNET = None, CNET = None):\n",
    "    rho, sigma = rhosigma\n",
    "    # print('DEBUG derivable_custom_pbe_e: rhosigma len/shapes: ', len(rhosigma), rhosigma)\n",
    "    # print('DEBUG derivable_custom_pbe_e: rho/sigma shapes: ', rho.shape, sigma.shape)\n",
    "    # print('DEBUG derivable_custom_pbe_e: rho: ', rho)\n",
    "    # print('DEBUG derivable_custom_pbe_e: sigma: ', sigma)\n",
    "    return custom_pbe_e(rho, sigma, XNET = XNET, CNET = CNET)\n",
    "\n",
    "def derivable_custom_pbe_epsilon(rhosigma, XNET = None, CNET = None):\n",
    "    rho = rhosigma[0]\n",
    "    sigma = rhosigma[1]\n",
    "    result = custom_pbe_epsilon(rho, sigma, XNET = XNET, CNET = CNET)\n",
    "    return result[0]\n",
    "    \n",
    "def eval_xc_gga_j(xc_code, rho, spin=0, relativity=0, deriv=1, omega=None, verbose=None,\n",
    "                 XNET = None, CNET = None):\n",
    "    #we only expect there to be a rho0 array, but I unpack it as (rho, deriv) here to be in line with the\n",
    "    #pyscf example -- the size of the 'rho' array depends on the xc type (LDA, GGA, etc.)\n",
    "    #so since LDA calculation, check for size first.\n",
    "    rho0, dx, dy, dz = rho[:4]\n",
    "    rho0 = jnp.array(rho0)\n",
    "    sigma = jnp.array(dx**2+dy**2+dz**2)\n",
    "    # print('DEBUG eval_xc_gga_j: rho0/sigma shapes: ', rho0.shape, sigma.shape)\n",
    "    rhosig = (rho0, sigma)\n",
    "    #calculate the \"custom\" energy with rho -- THIS IS e\n",
    "    #cast back to np.array since that's what pyscf works with\n",
    "    #pass as tuple -- (rho, sigma)\n",
    "    derivable_net_e = partial(derivable_custom_pbe_e, XNET = XNET, CNET = CNET)\n",
    "    derivable_net_epsilon = partial(derivable_custom_pbe_epsilon, XNET = XNET, CNET = CNET)\n",
    "    exc = np.array(jax.vmap(derivable_net_e)( rhosig ) )\n",
    "    \n",
    "    #first order derivatives w.r.t. rho and sigma\n",
    "    vrho_f = eqx.filter_grad(derivable_net_epsilon)\n",
    "    vrhosigma = np.array(jax.vmap(vrho_f)( rhosig ))\n",
    "    # print('vrhosigma shape:', vrhosigma.shape)\n",
    "    vxc = (vrhosigma[0], vrhosigma[1], None, None)\n",
    "\n",
    "    # v2_f = eqx.filter_hessian(derivable_custom_pbe_epsilon)\n",
    "    v2_f = jax.hessian(derivable_net_epsilon)\n",
    "    # v2_f = jax.hessian(custom_pbe_epsilon, argnums=[0, 1])\n",
    "    v2 = np.array(jax.vmap(v2_f)( rhosig ))\n",
    "    # print('v2 shape', v2.shape)\n",
    "    v2rho2 = v2[0][0]\n",
    "    v2rhosigma = v2[0][1]\n",
    "    v2sigma2 = v2[1][1]\n",
    "    v2lapl2 = None\n",
    "    vtau2 = None\n",
    "    v2rholapl = None\n",
    "    v2rhotau = None\n",
    "    v2lapltau = None\n",
    "    v2sigmalapl = None\n",
    "    v2sigmatau = None\n",
    "    # 2nd order functional derivative\n",
    "    fxc = (v2rho2, v2rhosigma, v2sigma2, v2lapl2, vtau2, v2rholapl, v2rhotau, v2lapltau, v2sigmalapl, v2sigmatau)\n",
    "    #3rd order\n",
    "    kxc = None\n",
    "    \n",
    "    return exc, vxc, fxc, kxc\n",
    "\n",
    "def eval_xc_gga_j2(xc_code, rho, spin=0, relativity=0, deriv=1, omega=None, verbose=None,\n",
    "                 xcmodel = None):\n",
    "    #we only expect there to be a rho0 array, but I unpack it as (rho, deriv) here to be in line with the\n",
    "    #pyscf example -- the size of the 'rho' array depends on the xc type (LDA, GGA, etc.)\n",
    "    #so since LDA calculation, check for size first.\n",
    "    try:\n",
    "        rho0, dx, dy, dz = rho[:4]\n",
    "        sigma = jnp.array(dx**2+dy**2+dz**2)\n",
    "    except:\n",
    "        rho0, drho = rho[:4]\n",
    "        sigma = jnp.array(drho**2)\n",
    "    rho0 = jnp.array(rho0)\n",
    "    # sigma = jnp.array(dx**2+dy**2+dz**2)\n",
    "    # print('DEBUG eval_xc_gga_j: rho0/sigma shapes: ', rho0.shape, sigma.shape)\n",
    "    # rhosig = (rho0, sigma)\n",
    "    rhosig = jnp.stack([rho0, sigma], axis=1)\n",
    "    # print(rhosig.shape)\n",
    "    #calculate the \"custom\" energy with rho -- THIS IS e\n",
    "    #cast back to np.array since that's what pyscf works with\n",
    "    #pass as tuple -- (rho, sigma)\n",
    "    exc = jax.vmap(xcmodel)(rhosig)\n",
    "    exc = jnp.array(exc)/rho0\n",
    "    # exc = jnp.array(jax.vmap(xcmodel)( rhosig ) )/rho0\n",
    "    # print('exc shape = {}'.format(exc.shape))\n",
    "    #first order derivatives w.r.t. rho and sigma\n",
    "    vrho_f = eqx.filter_grad(xcmodel)\n",
    "    vrhosigma = jnp.array(jax.vmap(vrho_f)( rhosig ))\n",
    "    # print('vrhosigma shape:', vrhosigma.shape)\n",
    "    vxc = (vrhosigma[:, 0], vrhosigma[:, 1], None, None)\n",
    "\n",
    "    # v2_f = eqx.filter_hessian(derivable_custom_pbe_epsilon)\n",
    "    v2_f = jax.hessian(xcmodel)\n",
    "    # v2_f = jax.hessian(custom_pbe_epsilon, argnums=[0, 1])\n",
    "    v2 = jnp.array(jax.vmap(v2_f)( rhosig ))\n",
    "    # print('v2 shape', v2.shape)\n",
    "    v2rho2 = v2[:, 0, 0]\n",
    "    v2rhosigma = v2[:, 0, 1]\n",
    "    v2sigma2 = v2[:, 1, 1]\n",
    "    v2lapl2 = None\n",
    "    vtau2 = None\n",
    "    v2rholapl = None\n",
    "    v2rhotau = None\n",
    "    v2lapltau = None\n",
    "    v2sigmalapl = None\n",
    "    v2sigmatau = None\n",
    "    # 2nd order functional derivative\n",
    "    fxc = (v2rho2, v2rhosigma, v2sigma2, v2lapl2, vtau2, v2rholapl, v2rhotau, v2lapltau, v2sigmalapl, v2sigmatau)\n",
    "    #3rd order\n",
    "    kxc = None\n",
    "    \n",
    "    return exc, vxc, fxc, kxc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: <pyscfad.gto.mole.Mole object at 0x7d69213d9840> must be initialized before calling SCF.\n",
      "Initialize <pyscfad.gto.mole.Mole object at 0x7d69213d9840> in RKS-KohnShamDFT object of <class 'pyscfad.dft.rks.RKS'>\n",
      "/home/ICN2/snavarro/.local/lib/python3.10/site-packages/pyscf/gto/mole.py:1215: UserWarning: Function mol.dumps drops attribute coords because it is not JSON-serializable\n",
      "  warnings.warn(msg)\n",
      "/home/ICN2/snavarro/.local/lib/python3.10/site-packages/pyscf/gto/mole.py:1215: UserWarning: Function mol.dumps drops attribute exp because it is not JSON-serializable\n",
      "  warnings.warn(msg)\n",
      "/home/ICN2/snavarro/.local/lib/python3.10/site-packages/pyscf/gto/mole.py:1215: UserWarning: Function mol.dumps drops attribute ctr_coeff because it is not JSON-serializable\n",
      "  warnings.warn(msg)\n",
      "/home/ICN2/snavarro/.local/lib/python3.10/site-packages/pyscfad/_src/util.py:108: UserWarning: Not taking derivatives wrt the leaves in the node <class 'pyscfad.dft.rks.VXC'> as none of those was specified.\n",
      "  warnings.warn(f'Not taking derivatives wrt the leaves in '\n",
      "/home/ICN2/snavarro/.local/lib/python3.10/site-packages/pyscf/gto/mole.py:1215: UserWarning: Function mol.dumps drops attribute coords because it is not JSON-serializable\n",
      "  warnings.warn(msg)\n",
      "/home/ICN2/snavarro/.local/lib/python3.10/site-packages/pyscf/gto/mole.py:1215: UserWarning: Function mol.dumps drops attribute exp because it is not JSON-serializable\n",
      "  warnings.warn(msg)\n",
      "/home/ICN2/snavarro/.local/lib/python3.10/site-packages/pyscf/gto/mole.py:1215: UserWarning: Function mol.dumps drops attribute ctr_coeff because it is not JSON-serializable\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "mol = gto_ad.Mole(\n",
    "    atom = '''\n",
    "    O  0.   0.       0.\n",
    "    H  0.   -0.757   0.587\n",
    "    H  0.   0.757    0.587 ''',\n",
    "    basis = 'ccpvdz')\n",
    "mol.verbose = 0\n",
    "mol.max_memory = 32000\n",
    "mf = dft_ad.RKS(mol)\n",
    "mf.grids.level = 1\n",
    "mf.xc = 'pbe'\n",
    "mf.kernel()\n",
    "start_hf = scf.RHF(mol)\n",
    "start_hf.kernel()\n",
    "cc_h2o = cc.CCSD(start_hf)\n",
    "cc_h2o.kernel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate CCSD density using DFT coordinates and CCSD density matrix.\n",
    "ao = dft.numint.eval_ao(mol, mf.grids.coords, deriv=2)\n",
    "CCSD_COORDS = np.array(mf.grids.coords)\n",
    "ccsd_dm = cc_h2o.make_rdm1()\n",
    "rho = dft_ad.numint.eval_rho(mol, jnp.array(ao), jnp.array(ccsd_dm), xctype='MGGA')\n",
    "target_rho_CCSD = jnp.array(rho[0]).flatten()\n",
    "TARGET_ENERGY = cc_h2o.e_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@eqx.filter_value_and_grad\n",
    "def opt_loss_TE_rho(model, mols, refs):\n",
    "    WEIGHT_E = 1.0\n",
    "    WEIGHT_RHO = 20.0\n",
    "    total_loss = 0\n",
    "    for idx, mol in enumerate(mols):\n",
    "        mf = dft_ad.RKS(mol)\n",
    "        custom_eval_xc = partial(eval_xc_gga_j2, xcmodel=model)\n",
    "        mf.grids.level = 1\n",
    "        mf.define_xc_(custom_eval_xc, 'GGA')\n",
    "        mf.kernel()\n",
    "        ao = dft.numint.eval_ao(mol, mf.grids.coords, deriv=1)\n",
    "        dm = mf.make_rdm1()\n",
    "        rho = dft_ad.numint.eval_rho(mol, jnp.array(ao), dm, xctype='GGA')[0].flatten()\n",
    "        loss_E = abs(mf.e_tot - refs[idx][0])\n",
    "        print(refs[idx][0])\n",
    "        print(refs[idx][1])\n",
    "        loss_rho = (1/mol.nelectron)*jnp.sum(mf.grids.weights*jnp.sqrt((rho - refs[idx][1])**2))\n",
    "        total_loss += WEIGHT_E * loss_E + WEIGHT_RHO * loss_rho\n",
    "    total_loss /= len(mols)\n",
    "    return total_loss[..., jnp.newaxis][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RXCModel(eqx.Module):\n",
    "    xnet: eqx.Module\n",
    "    cnet: eqx.Module\n",
    "\n",
    "    def __init__(self, xnet, cnet):\n",
    "        self.xnet = xnet\n",
    "        self.cnet = cnet\n",
    "        \n",
    "    def __call__(self, inputs):\n",
    "        #this generate epsilon, not exc -- divide end result by rho when needed\n",
    "        rho = inputs[0]\n",
    "        sigma = inputs[1]\n",
    "        # print('RXCModel call - inputs {}'.format(inputs))\n",
    "        return rho*(lda_x(rho)*jax.vmap(self.xnet)(inputs[..., jnp.newaxis]) + pw92c_unpolarized(rho)*jax.vmap(self.cnet)(inputs[..., jnp.newaxis])).flatten()[0]\n",
    "        # return rho*(lda_x(rho)*self.xnet(inputs) + pw92c_unpolarized(rho)*self.cnet(inputs)).flatten()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "class Optimizer(eqx.Module):\n",
    "    model: eqx.Module\n",
    "    optim: optax.GradientTransformation\n",
    "    steps: int\n",
    "    print_every: int\n",
    "    opt_state: tuple\n",
    "    mols: list\n",
    "    refs: jnp.array\n",
    "    loss: Callable\n",
    "\n",
    "    def __init__(self, model, optim, mols, refs, loss, steps=1000, print_every=100):\n",
    "        '''\n",
    "        The Pretrainer object aids in the initial pre-training of enhancement factor networks to have a more physical starting point for further network optimization. This class is meant to pre-train a randomly initialized network to fit the values of a specific XC functional's enhancement factor (either X or C, in principle it could also be a combined XC enhancement facator)\n",
    "\n",
    "        :param model: The enhancement factor network to be pre-trained\n",
    "        :type model: :xcquinox.net: class\n",
    "        :param optim: The optimizer than will control the weight updates given a loss and gradient\n",
    "        :type optim: optax.GradientTransformation\n",
    "        :param inputs: The inputs the network itself is expecting in its forward pass function\n",
    "        :type inputs: jnp.array\n",
    "        :param ref: The reference values the network is expected to reproduce\n",
    "        :type ref: jnp.array\n",
    "        :param loss: A function from :xcquinox.loss: that is decorated with @eqx.filter_value_and_grad\n",
    "        :type loss: Callable\n",
    "        :param steps: Number of epochs to train over, defaults to 1000\n",
    "        :type steps: int, optional\n",
    "        :param print_every: How often to print loss statistic, defaults to 100\n",
    "        :type print_every: int, optional\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.optim = optim\n",
    "        self.mols = mols\n",
    "        self.refs = refs\n",
    "        self.steps = steps\n",
    "        self.print_every = print_every\n",
    "        self.opt_state = self.optim.init(eqx.filter(self.model, eqx.is_array))\n",
    "        self.loss = loss\n",
    "\n",
    "    def __call__(self):\n",
    "        '''\n",
    "        The training loop itself. Here, a loop over the specifed epochs takes place to train the network to fit reference values.\n",
    "\n",
    "        :return: The trained model and an array of the losses during training.\n",
    "        :rtype: (:xcquinox.net: class, array)\n",
    "        '''\n",
    "        losses = []\n",
    "        for epoch in range(self.steps):\n",
    "            if epoch == 0:\n",
    "                this_model = self.model\n",
    "                this_opt_state = self.opt_state\n",
    "            loss, this_model, this_opt_state = self.make_step(this_model, self.mols, self.refs, this_opt_state)\n",
    "            lossi = loss.item()\n",
    "            losses.append(lossi)\n",
    "            if epoch % self.print_every == 0:\n",
    "                print(f'Epoch {epoch}: Loss = {lossi}')\n",
    "\n",
    "        return this_model, losses\n",
    "\n",
    "    # @eqx.filter_jit\n",
    "    def make_step(self, model, inputs, ref, opt_state):\n",
    "        '''\n",
    "        The function that does each epoch's network update. It generates a loss and gradient using the specific :xcquinox.loss: function (that must be decorated with @eqx.filter_value_and_grad and only explicitly returns the loss value inside the function proper) given the specified inputs and reference values and initial optimization state.\n",
    "\n",
    "        :param model: The enhancement factor network to be pre-trained\n",
    "        :type model: :xcquinox.net: class\n",
    "        :param inputs: The inputs the network itself is expecting in its forward pass function\n",
    "        :type inputs: jnp.array\n",
    "        :param ref: The reference values the network is expected to reproduce\n",
    "        :type ref: jnp.array\n",
    "        :param opt_state: The INITIAL optimization state to work against, typically generated via :self.optim.init(eqx.filter(self.model, eqx.is_array)):\n",
    "        :type opt_state: The type of the above\n",
    "        :return: The loss value for this step, the updated model after that loss is calculated, and the new optimization state for this step to use next time\n",
    "        :rtype: tuple\n",
    "        '''\n",
    "        loss, grad = self.loss(model, self.mols, self.refs)\n",
    "        updates, opt_state = self.optim.update(grad, opt_state)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return loss, model, opt_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = RXCModel(xnet=snmx, cnet=snmc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mol_ad = gto_ad.Mole(\n",
    "    atom = '''\n",
    "    O  0.   0.       0.\n",
    "    H  0.   -0.757   0.587\n",
    "    H  0.   0.757    0.587 ''',\n",
    "    basis = 'ccpvdz')\n",
    "mol_ad.max_memory = 32000\n",
    "mol_ad.build()\n",
    "mol_ad.verbose = 0\n",
    "OPT_INIT_LR = 1e-2\n",
    "OPT_END_LR = 1e-5\n",
    "OPTSTEPS = 100\n",
    "OPTDECAYBEGIN = 30\n",
    "# OPTDECAYRATE = 0.95\n",
    "scheduler = optax.linear_schedule(\n",
    "    init_value = OPT_INIT_LR,\n",
    "    transition_steps = OPTSTEPS-OPTDECAYBEGIN,\n",
    "    transition_begin = OPTDECAYBEGIN,\n",
    "    end_value = OPT_END_LR,\n",
    ")\n",
    "opt_opt = optax.adam(learning_rate=scheduler)\n",
    "opt_snmxc = Optimizer(model=test_model, optim=opt_opt, mols = [mol_ad], refs = [[TARGET_ENERGY, target_rho_CCSD]], loss=opt_loss_TE_rho, print_every=1, steps=OPTSTEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#a lot of \"derivative wrt the leaves not taken\" printouts from pyscfad -- suppress that so we can actually see the losses easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-76.24010891900583\n",
      "[1.00926408e-11 3.81612990e-13 2.75044280e-12 ... 6.25946208e-10\n",
      " 5.35069843e-12 5.35212076e-12]\n",
      "Epoch 0: Loss = 8.835664419074323\n",
      "-76.24010891900583\n",
      "[1.00926408e-11 3.81612990e-13 2.75044280e-12 ... 6.25946208e-10\n",
      " 5.35069843e-12 5.35212076e-12]\n",
      "Epoch 1: Loss = 8.84904046454508\n",
      "-76.24010891900583\n",
      "[1.00926408e-11 3.81612990e-13 2.75044280e-12 ... 6.25946208e-10\n",
      " 5.35069843e-12 5.35212076e-12]\n",
      "Epoch 2: Loss = 8.850540876242775\n",
      "-76.24010891900583\n",
      "[1.00926408e-11 3.81612990e-13 2.75044280e-12 ... 6.25946208e-10\n",
      " 5.35069843e-12 5.35212076e-12]\n",
      "Epoch 3: Loss = 8.846817154453063\n",
      "-76.24010891900583\n",
      "[1.00926408e-11 3.81612990e-13 2.75044280e-12 ... 6.25946208e-10\n",
      " 5.35069843e-12 5.35212076e-12]\n",
      "Epoch 4: Loss = 8.840016143162007\n",
      "-76.24010891900583\n",
      "[1.00926408e-11 3.81612990e-13 2.75044280e-12 ... 6.25946208e-10\n",
      " 5.35069843e-12 5.35212076e-12]\n",
      "Epoch 5: Loss = 8.831164498774864\n",
      "-76.24010891900583\n",
      "[1.00926408e-11 3.81612990e-13 2.75044280e-12 ... 6.25946208e-10\n",
      " 5.35069843e-12 5.35212076e-12]\n",
      "Epoch 6: Loss = 8.820984760096282\n",
      "-76.24010891900583\n",
      "[1.00926408e-11 3.81612990e-13 2.75044280e-12 ... 6.25946208e-10\n",
      " 5.35069843e-12 5.35212076e-12]\n",
      "Epoch 7: Loss = 8.819832360540007\n",
      "-76.24010891900583\n",
      "[1.00926408e-11 3.81612990e-13 2.75044280e-12 ... 6.25946208e-10\n",
      " 5.35069843e-12 5.35212076e-12]\n",
      "Epoch 8: Loss = 8.822998232569393\n",
      "-76.24010891900583\n",
      "[1.00926408e-11 3.81612990e-13 2.75044280e-12 ... 6.25946208e-10\n",
      " 5.35069843e-12 5.35212076e-12]\n",
      "Epoch 9: Loss = 8.828892297050404\n",
      "-76.24010891900583\n",
      "[1.00926408e-11 3.81612990e-13 2.75044280e-12 ... 6.25946208e-10\n",
      " 5.35069843e-12 5.35212076e-12]\n",
      "Epoch 10: Loss = 8.832631013641485\n",
      "-76.24010891900583\n",
      "[1.00926408e-11 3.81612990e-13 2.75044280e-12 ... 6.25946208e-10\n",
      " 5.35069843e-12 5.35212076e-12]\n",
      "Epoch 11: Loss = 8.83472899796202\n",
      "-76.24010891900583\n",
      "[1.00926408e-11 3.81612990e-13 2.75044280e-12 ... 6.25946208e-10\n",
      " 5.35069843e-12 5.35212076e-12]\n",
      "Epoch 12: Loss = 8.835594502522351\n",
      "-76.24010891900583\n",
      "[1.00926408e-11 3.81612990e-13 2.75044280e-12 ... 6.25946208e-10\n",
      " 5.35069843e-12 5.35212076e-12]\n",
      "Epoch 13: Loss = 8.837509850095703\n",
      "-76.24010891900583\n",
      "[1.00926408e-11 3.81612990e-13 2.75044280e-12 ... 6.25946208e-10\n",
      " 5.35069843e-12 5.35212076e-12]\n",
      "Epoch 14: Loss = 8.838510785451586\n",
      "-76.24010891900583\n",
      "[1.00926408e-11 3.81612990e-13 2.75044280e-12 ... 6.25946208e-10\n",
      " 5.35069843e-12 5.35212076e-12]\n",
      "Epoch 15: Loss = 8.83880578424396\n",
      "-76.24010891900583\n",
      "[1.00926408e-11 3.81612990e-13 2.75044280e-12 ... 6.25946208e-10\n",
      " 5.35069843e-12 5.35212076e-12]\n",
      "Epoch 16: Loss = 8.839627255718892\n",
      "-76.24010891900583\n",
      "[1.00926408e-11 3.81612990e-13 2.75044280e-12 ... 6.25946208e-10\n",
      " 5.35069843e-12 5.35212076e-12]\n",
      "Epoch 17: Loss = 8.839866222061271\n",
      "-76.24010891900583\n",
      "[1.00926408e-11 3.81612990e-13 2.75044280e-12 ... 6.25946208e-10\n",
      " 5.35069843e-12 5.35212076e-12]\n",
      "Epoch 18: Loss = 8.8396222888432\n",
      "-76.24010891900583\n",
      "[1.00926408e-11 3.81612990e-13 2.75044280e-12 ... 6.25946208e-10\n",
      " 5.35069843e-12 5.35212076e-12]\n",
      "Epoch 19: Loss = 8.839802896318698\n",
      "-76.24010891900583\n",
      "[1.00926408e-11 3.81612990e-13 2.75044280e-12 ... 6.25946208e-10\n",
      " 5.35069843e-12 5.35212076e-12]\n",
      "Epoch 20: Loss = 8.84050648452012\n",
      "-76.24010891900583\n",
      "[1.00926408e-11 3.81612990e-13 2.75044280e-12 ... 6.25946208e-10\n",
      " 5.35069843e-12 5.35212076e-12]\n",
      "Epoch 21: Loss = 8.840589515515362\n",
      "-76.24010891900583\n",
      "[1.00926408e-11 3.81612990e-13 2.75044280e-12 ... 6.25946208e-10\n",
      " 5.35069843e-12 5.35212076e-12]\n",
      "Epoch 22: Loss = 8.840033424962664\n",
      "-76.24010891900583\n",
      "[1.00926408e-11 3.81612990e-13 2.75044280e-12 ... 6.25946208e-10\n",
      " 5.35069843e-12 5.35212076e-12]\n",
      "Epoch 23: Loss = 8.83884852204381\n",
      "-76.24010891900583\n",
      "[1.00926408e-11 3.81612990e-13 2.75044280e-12 ... 6.25946208e-10\n",
      " 5.35069843e-12 5.35212076e-12]\n",
      "Epoch 24: Loss = 8.837065807216913\n",
      "-76.24010891900583\n",
      "[1.00926408e-11 3.81612990e-13 2.75044280e-12 ... 6.25946208e-10\n",
      " 5.35069843e-12 5.35212076e-12]\n",
      "Epoch 25: Loss = 8.834727859465739\n",
      "-76.24010891900583\n",
      "[1.00926408e-11 3.81612990e-13 2.75044280e-12 ... 6.25946208e-10\n",
      " 5.35069843e-12 5.35212076e-12]\n",
      "Epoch 26: Loss = 8.831881668379037\n"
     ]
    }
   ],
   "source": [
    "osnmxc, osnmxcl = opt_snmxc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-76.24010891900579, Array([1.00926408e-11, 3.81612990e-13, 2.75044280e-12, ...,\n",
      "       3.46968228e+02, 3.46968228e+02, 3.46968228e+02], dtype=float64)]\n",
      "-76.24010891900579\n"
     ]
    }
   ],
   "source": [
    "refs = [TARGET_ENERGY, target_rho_CCSD]\n",
    "print(refs)\n",
    "print(refs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_xcq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
